{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"kernelVersion","sourceId":160765215}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-01-28T17:29:26.550527Z\",\"iopub.execute_input\":\"2024-01-28T17:29:26.551394Z\",\"iopub.status.idle\":\"2024-01-28T17:29:26.557817Z\",\"shell.execute_reply.started\":\"2024-01-28T17:29:26.551361Z\",\"shell.execute_reply\":\"2024-01-28T17:29:26.556741Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-01-28T17:29:26.559380Z\",\"iopub.execute_input\":\"2024-01-28T17:29:26.559733Z\",\"iopub.status.idle\":\"2024-01-28T17:29:26.572410Z\",\"shell.execute_reply.started\":\"2024-01-28T17:29:26.559706Z\",\"shell.execute_reply\":\"2024-01-28T17:29:26.571444Z\"}}\nimport torch\nimport model\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-01-28T17:29:26.574267Z\",\"iopub.execute_input\":\"2024-01-28T17:29:26.574693Z\",\"iopub.status.idle\":\"2024-01-28T17:29:26.584445Z\",\"shell.execute_reply.started\":\"2024-01-28T17:29:26.574660Z\",\"shell.execute_reply\":\"2024-01-28T17:29:26.583694Z\"}}\nfrom torch.autograd import Variable\n\nnum_epochs = 10\n\ndef train(num_epochs, cnn, loaders):\n    model.cnn.train()\n    \n    # Train the model\n    total_step = len(loaders['train'])\n    \n    #losslist=[]\n    #epochlist=[]\n    for epoch in range(num_epochs):\n        for i, (images, labels) in enumerate(loaders['train']):\n            \n            # gives batch data, normalize x when iterate train_loader\n            b_x = Variable(images)   # batch x\n            b_y = Variable(labels)   # batch y\n            output = cnn(b_x)[0]\n            loss = model.loss(output, b_y)\n            \n            # clear gradients for this training step\n            model.optimizer.zero_grad()\n            \n            # backpropagation, compute gradients\n            loss.backward()\n            \n            # apply gradients\n            model.optimizer.step()\n            \n            if (i+1) % 100 == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n            #epochlist.append(epoch + 1)\n            #losslist.append(loss.item())   \n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-01-28T17:29:26.585445Z\",\"iopub.execute_input\":\"2024-01-28T17:29:26.585717Z\",\"iopub.status.idle\":\"2024-01-28T17:29:40.867014Z\",\"shell.execute_reply.started\":\"2024-01-28T17:29:26.585696Z\",\"shell.execute_reply\":\"2024-01-28T17:29:40.865653Z\"}}\n \n    \ntrain(num_epochs, model.cnn, model.loaders)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-01-28T17:29:40.868002Z\",\"iopub.status.idle\":\"2024-01-28T17:29:40.868349Z\",\"shell.execute_reply.started\":\"2024-01-28T17:29:40.868181Z\",\"shell.execute_reply\":\"2024-01-28T17:29:40.868197Z\"}}\n#plt.plot(epochlist, losslist, label='Training Loss')\n#plt.xlabel('Epoch')\n#plt.ylabel('Loss')\n#plt.title('Loss vs Epoch')\n#plt.legend()\n#plt.show()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n","metadata":{"_uuid":"db3af2ad-0de0-4f1e-93bf-76a782186673","_cell_guid":"bbc945ee-a874-4379-8fb3-82ddfff00f6c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-28T17:29:52.196537Z","iopub.execute_input":"2024-01-28T17:29:52.196930Z","iopub.status.idle":"2024-01-28T17:31:47.622138Z","shell.execute_reply.started":"2024-01-28T17:29:52.196902Z","shell.execute_reply":"2024-01-28T17:31:47.621047Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch [1/10], Step [100/480], Loss: 0.0000\nEpoch [1/10], Step [200/480], Loss: 0.2383\nEpoch [1/10], Step [300/480], Loss: 0.0000\nEpoch [1/10], Step [400/480], Loss: 0.0007\nEpoch [2/10], Step [100/480], Loss: 0.0000\nEpoch [2/10], Step [200/480], Loss: 0.0000\nEpoch [2/10], Step [300/480], Loss: 0.0000\nEpoch [2/10], Step [400/480], Loss: 0.0000\nEpoch [3/10], Step [100/480], Loss: 0.3304\nEpoch [3/10], Step [200/480], Loss: 0.0374\nEpoch [3/10], Step [300/480], Loss: 0.0000\nEpoch [3/10], Step [400/480], Loss: 0.0000\nEpoch [4/10], Step [100/480], Loss: 0.0000\nEpoch [4/10], Step [200/480], Loss: 0.0000\nEpoch [4/10], Step [300/480], Loss: 0.0000\nEpoch [4/10], Step [400/480], Loss: 0.0000\nEpoch [5/10], Step [100/480], Loss: 0.0000\nEpoch [5/10], Step [200/480], Loss: 0.0000\nEpoch [5/10], Step [300/480], Loss: 0.0000\nEpoch [5/10], Step [400/480], Loss: 0.0431\nEpoch [6/10], Step [100/480], Loss: 0.0000\nEpoch [6/10], Step [200/480], Loss: 0.0000\nEpoch [6/10], Step [300/480], Loss: 0.0000\nEpoch [6/10], Step [400/480], Loss: 0.0000\nEpoch [7/10], Step [100/480], Loss: 0.0894\nEpoch [7/10], Step [200/480], Loss: 0.0000\nEpoch [7/10], Step [300/480], Loss: 0.0000\nEpoch [7/10], Step [400/480], Loss: 0.1576\nEpoch [8/10], Step [100/480], Loss: 0.0000\nEpoch [8/10], Step [200/480], Loss: 0.0062\nEpoch [8/10], Step [300/480], Loss: 0.0000\nEpoch [8/10], Step [400/480], Loss: 0.1006\nEpoch [9/10], Step [100/480], Loss: 0.0000\nEpoch [9/10], Step [200/480], Loss: 0.3721\nEpoch [9/10], Step [300/480], Loss: 0.2030\nEpoch [9/10], Step [400/480], Loss: 0.0000\nEpoch [10/10], Step [100/480], Loss: 0.0000\nEpoch [10/10], Step [200/480], Loss: 0.0000\nEpoch [10/10], Step [300/480], Loss: 0.0685\nEpoch [10/10], Step [400/480], Loss: 0.0000\n","output_type":"stream"}]}]}